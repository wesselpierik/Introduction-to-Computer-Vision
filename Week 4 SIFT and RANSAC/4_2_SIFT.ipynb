{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Utilities\" data-toc-modified-id=\"Utilities-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Utilities</a></span></li><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Projective-Transform-and-Warping\" data-toc-modified-id=\"Projective-Transform-and-Warping-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Projective Transform and Warping</a></span></li><li><span><a href=\"#Scale-Invariant-Feature-Transform-(SIFT)\" data-toc-modified-id=\"Scale-Invariant-Feature-Transform-(SIFT)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Scale Invariant Feature Transform (SIFT)</a></span><ul class=\"toc-item\"><li><span><a href=\"#SIFT-in-Theory\" data-toc-modified-id=\"SIFT-in-Theory-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>SIFT in Theory</a></span></li><li><span><a href=\"#SIFT-in-Practice\" data-toc-modified-id=\"SIFT-in-Practice-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>SIFT in Practice</a></span></li></ul></li><li><span><a href=\"#Matching-Descriptors\" data-toc-modified-id=\"Matching-Descriptors-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Matching Descriptors</a></span></li><li><span><a href=\"#Random-Sample-Concensus-(RANSAC)\" data-toc-modified-id=\"Random-Sample-Concensus-(RANSAC)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Random Sample Concensus (RANSAC)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ransac-for-Line-Fitting\" data-toc-modified-id=\"Ransac-for-Line-Fitting-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Ransac for Line Fitting</a></span></li><li><span><a href=\"#Find-a-Perspective-Transform-using-Ransac\" data-toc-modified-id=\"Find-a-Perspective-Transform-using-Ransac-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Find a Perspective Transform using Ransac</a></span></li></ul></li><li><span><a href=\"#Sztitching-it-all-together\" data-toc-modified-id=\"Stitching-it-all-together-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Stitching it all together</a></span></li><li><span><a href=\"#(Bonus)-A-Better-Stitch\" data-toc-modified-id=\"(Bonus)-A-Better-Stitch-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>(Bonus) A Better Stitch</a></span></li><li><span><a href=\"#(Bonus)-Stitching-your-own-Images\" data-toc-modified-id=\"(Bonus)-Stitching-your-own-Images-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>(Bonus) Stitching your own Images</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICV INF LabExercise: Image Stitching using SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wget\n",
    "from skimage.transform import warp\n",
    "import scipy\n",
    "import cv2\n",
    "from matplotlib.patches import ConnectionPatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we are working with SIFT. For this we need the right version of the `opencv` package. Did you do assignment 0 of week 1, about installing the correct version? If not, go do that now. You can try `conda install -c conda-forge opencv-contrib-python`,`python3 -m pip install --user opencv-contrib-python`, or `pip install opencv-python-headless`. If you have difficulties with installation, please refer to your TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden in this cell are some $\\LaTeX$ macros\n",
    "$\\newcommand{\\v}[1]{\\mathbf #1}$\n",
    "$\\newcommand{\\hv}[1]{\\widetilde{\\mathbf #1}}$\n",
    "$\\newcommand{\\setR}{\\mathbb R}$\n",
    "$\\newcommand{\\T}{^\\top}$\n",
    "$\\newcommand{\\inv}{^{-1}}$\n",
    "$\\newcommand{\\pfrac}[2]{\\frac{\\partial #1}{\\partial #2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section there is some code that is needed later on in this exercise. Make sure this code is executed before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def imshow_row(imttllist, axs=False):\n",
    "    n = len(imttllist)\n",
    "    for i, imttl in enumerate(imttllist):\n",
    "        if imttl is None:\n",
    "            continue\n",
    "        im, ttl = imttl\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.imshow(im, cmap='gray')\n",
    "        if not axs:\n",
    "            plt.axis('off')\n",
    "        plt.title(ttl)\n",
    "        \n",
    "def draw_keypoints(kps, nkps=None, ax=None, marker='x', \n",
    "                   marker_size=10, scale_and_orientation=True):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    if nkps is None:\n",
    "        nkps = len(kps)\n",
    "    xs = [kp.pt[0] for kp in kps]\n",
    "    ys = [kp.pt[1] for kp in kps]\n",
    "    ax.scatter(xs, ys, marker=marker)\n",
    "    if scale_and_orientation:\n",
    "        for kp in kps[:nkps]:\n",
    "            x, y = kp.pt\n",
    "            r = kp.size / 2\n",
    "            angle = kp.angle/2/np.pi\n",
    "            ax.add_artist(plt.Circle((kp.pt), kp.size/2, \n",
    "                                     color='green', fill=False))\n",
    "            ax.add_artist(plt.Arrow(x, y, r*np.cos(angle), \n",
    "                                    r*np.sin(angle), color='red'))\n",
    "            \n",
    "\n",
    "\n",
    "def draw_matches(f1, kps1, f2, kps2, matches, \n",
    "                 horizontal=True, figsize=(15,15)):\n",
    "    if horizontal:\n",
    "        fig, axs = plt.subplots(1,2, figsize=figsize)\n",
    "    else:\n",
    "        fig, axs = plt.subplots(2,1, figsize=figsize)\n",
    "    \n",
    "    axs[0].imshow(f1)\n",
    "    axs[1].imshow(f2)\n",
    "    \n",
    "    # get the indexes of the matches\n",
    "    idx1 = [m.queryIdx for m in matches]\n",
    "    idx2 = [m.trainIdx for m in matches]\n",
    "    \n",
    "    xs1 = [kps1[i].pt[0] for i in idx1]\n",
    "    ys1 = [kps1[i].pt[1] for i in idx1]\n",
    "    xs2 = [kps2[i].pt[0] for i in idx2]\n",
    "    ys2 = [kps2[i].pt[1] for i in idx2]\n",
    "    \n",
    "    \n",
    "    for x1, y1, x2, y2 in zip(xs1, ys1, xs2, ys2):\n",
    "        con = ConnectionPatch(xyA=(x1, y1), xyB=(x2, y2), coordsA=\"data\", coordsB=\"data\",\n",
    "                      axesA=axs[0], axesB=axs[1], color='g')\n",
    "        axs[1].add_artist(con)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to write the software to stitch two or more images of the same scene together. In this report we describe the process of stitching only two images together. The basic steps of the algorithm are:\n",
    " \n",
    "1. **Locate맒eypoints**말n both images. A keypoint is defined by its맗osition$(x,y)$ in the image, the맙cale$s$마t which it is detected, its맖rientation$\\theta$마nd a맋escriptor$\\v d$맚hat characterizes what the image looks like in the neighborhood of the keypoint.\n",
    "\n",
    "1. **Match맚he keypoints** in both images. A pair$((x,y,\\theta,\\v d), (x',y',\\theta',\\v d')$ (the primed keypoint is in image 2, the other keypoint is in image 1) matches when$\\|\\v d - \\v d'\\|$ is small enough to conclude that image 1 around point$(x,y)$말s visually (almost) equal to image 2 around point$(x', y')$.\n",
    "\n",
    "1. **Estimate the transformation**$P$맚hat maps points in image 1 to the corresponding points in image 2. Because not all found point matches from step 2 are correct we need a robust estimation procedure that is capable of selecting those matching pairs that are indeed related through the transform$P$. For this we will use the RANSAC procedure.\n",
    "\n",
    "1. **Warp**막oth images to a common coordinate system, and display the result.\n",
    "\n",
    "This is an assignment in which the SIFT part can be used from OpenCV (it would be too big), but RANSAC you are asked to implement yourself (not too hard, and instructive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we start with stitching two small views on Rembrandt's Nachtwacht into a common coordinate frame. The two images are stored as jpeg files on disk. So when reading these images they are represented as (M,N,3) shaped arrays of dtype=uint8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f1 = plt.imread('nachtwacht1.jpg')\n",
    "except FileNotFoundError:\n",
    "    wget.download(\"https://staff.fnwi.uva.nl/r.vandenboomgaard/ComputerVision/_images/nachtwacht1.jpg\")\n",
    "    f1 = plt.imread('nachtwacht1.jpg')\n",
    "try:\n",
    "    f2 = plt.imread('nachtwacht2.jpg')\n",
    "except FileNotFoundError:\n",
    "    wget.download(\"https://staff.fnwi.uva.nl/r.vandenboomgaard/ComputerVision/_images/nachtwacht2.jpg\")\n",
    "    f2 = plt.imread('nachtwacht2.jpg')\n",
    "plt.figure(figsize=(15,15))\n",
    "imshow_row([(f1, \"Nachtwacht 1\"),\n",
    "            (f2, \"Nachtwacht 2\")])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the scale of both images is different and also the orientation is slightly different. In fact, it will turn out that we really need a projective transform to relate the coordinate frames of these two images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will proceed as follows:\n",
    "\n",
    "1. Using 4 and 6 point correspondences in the images we test your \n",
    "   implementation of ``projective_transform_matrix``\n",
    "\n",
    "1. Instead of manually selected point correspondences in both images \n",
    "   we will use SIFT and RANSAC to find the point correspondences \n",
    "   while at the same time estimating the projective transform.\n",
    "   \n",
    "   1. For SIFT we will use the OpenCV version, and\n",
    "   \n",
    "   1. For RANSAC you will have to implement your own version.\n",
    "   \n",
    "1. Then you have to make an image filled with zeros and warp both \n",
    "   nachtwacht images to the coordinate frame of the resulting image. \n",
    "   The new image should contain the two nachtwacht images completely \n",
    "   (note that this last requirement is not met within the first part \n",
    "   of this exercise)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Projective Transform and Warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the nachtwacht1 and nachtwacht2 image we have manually picked 4 corresponding points, these are collected in the arrays ``xy`` and ``xaya``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xy = np.array([[ 157, 32],\n",
    "               [ 211, 37],\n",
    "               [ 222,107],\n",
    "               [ 147,124]])\n",
    "xaya = np.array([[  6, 38],\n",
    "                 [ 56, 31],\n",
    "                 [ 82, 87],\n",
    "                 [ 22,118]])\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(f1)\n",
    "plt.scatter(xy[:,0], xy[:,1], marker='x', color='r')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(f2)\n",
    "plt.scatter(xaya[:,0], xaya[:,1], marker='x', color='r')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From these point correpondences you can calculate the projective transform $P$ that maps coordinates in the first image to the corresponding coordinates in the right image. We will be using the functions ``projective_transform_matrix``, `e2h`, `h2e` from previous labs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f149e7987f666c7d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def projective_transform_matrix(x1, x2, x3, x4, xa1, xa2, xa3, xa4):\n",
    "    M = np.empty((0,9))\n",
    "    for (x, y), (xa, ya) in zip([x1, x2, x3, x4], [xa1, xa2, xa3, xa4]):\n",
    "        M = np.append(M, \n",
    "                      np.array([[x, y, 1, 0, 0, 0, -xa*x, -xa*y, -xa],\n",
    "                                [0, 0, 0, x, y, 1, -ya*x, -ya*y, -ya]]),\n",
    "                      axis=0)\n",
    "    _, _, VT = np.linalg.svd(M)\n",
    "    p = VT[-1]\n",
    "    P = p.reshape((3,3))\n",
    "    return P\n",
    "\n",
    "def e2h(x):\n",
    "    if len(x.shape) == 1:\n",
    "        return np.hstack((x, [1]))\n",
    "    return np.vstack((x, np.ones(x.shape[1])))\n",
    "    \n",
    "def h2e(tx):\n",
    "    return tx[:-1]/tx[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "P = projective_transform_matrix(xy[0], xy[1], xy[2], xy[3],\n",
    "                           xaya[0], xaya[1], xaya[2], xaya[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_stitched = warp(f2, P, output_shape=(300,450), preserve_range=True).astype(np.uint8)\n",
    "plt.imshow(f_stitched);\n",
    "plt.axis('off');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "M, N = f1.shape[:2]\n",
    "f_stitched[:M, :N, :] = f1\n",
    "plt.imshow(f_stitched);\n",
    "plt.axis('off');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Be sure to understand what is going on in the code above. Can you explain why you didn't need the inverse of $P$ in the warp function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that the stitching is not exactly perfect. You can clearly see the border of the first image. The reason is that our hand-picked points weren't perfect matches for the two images. We can improve by using more than four points to get a best fit for the handpicked correspondences. \n",
    "\n",
    "You have to change your ``projective_transform_matrix`` to be able to do the estimation of the projection matrix in case more than the minimal 4 point correspondences are given. Below we give 6 point correspondences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xy6 = np.array([[ 157, 32],\n",
    "               [ 211, 37],\n",
    "               [ 222,107],\n",
    "               [ 147,124],\n",
    "               [ 163, 68],\n",
    "               [ 191,111]])\n",
    "xaya6 = np.array([[  6, 38],\n",
    "                 [ 56, 31],\n",
    "                 [ 82, 87],\n",
    "                 [ 22,118],\n",
    "                 [ 21, 66],\n",
    "                 [ 56, 96]])\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(f1)\n",
    "plt.scatter(xy6[:,0], xy6[:,1], marker='x', color='r')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(f2)\n",
    "plt.scatter(xaya6[:,0], xaya6[:,1], marker='x', color='r')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The function `projective_transform_matrix`, with as inputs four point correspondences, gives the unique projective mapping that maps those four point correspondences. Now you have to generalize the function: if you have more point correspondences, give the projective transformation which gives the closest possible approximation to the target points.\n",
    "\n",
    "We call this function `projective_fit_model`, and have it take a single parameter `data`, which is an iterable of pairs of points -- these choices will make sense later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-29fee09ba1460334",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def projective_fit_model(data):\n",
    "    \"\"\"\n",
    "    data is an iterable of pairs (input, output). Each pair consists of two Euclidean 2d vectors.\n",
    "    \n",
    "    Calculate the projection matrix P that maps all input points onto their corresponding output point,\n",
    "    or as close as possible.\n",
    "    \n",
    "    data will have at least four pairs. If it has exactly four pairs it should behave the same as\n",
    "    `projective_transform_matrix` (possibly up to a multiplicative constant):\n",
    "    \n",
    "        projective_transform_matrix(x1, x2, x3, x4, xa1, xa2, xa3, xa4) ==\n",
    "            projective_fit_model([(x1, xa1), (x2, xa2), (x3, xa3), (x4, xa4)])\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE (Replace this and the following line with your code)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "P6 = projective_fit_model(zip(xy, xaya))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_stitched6 = warp(f2, P6, output_shape=(300,450))\n",
    "plt.imshow(f_stitched6);\n",
    "plt.axis('off');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "M, N = f1.shape[:2]\n",
    "f_stitched6[:M, :N, :] = f1/255.0 #rescaling f1 to play nice with the warp function\n",
    "plt.imshow(f_stitched6);\n",
    "plt.axis('off');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that even 6 point correspondences that are manually selected do not result in good result (look at the white hat at the border of both images). In the sequel of this lab exercise SIFT/RANSAC will be used to automatically find corresponding points in both images with much better accuracy leading to better stitching results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Invariant Feature Transform (SIFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the SIFT algorithm from OpenCV. A quite complex algorithm using quite some clever 'tricks-of-the-(computer vision)-trade'. In case you are interested look at the [source code](https://github.com/opencv/opencv_contrib/blob/master/modules/xfeatures2d/src/sift.cpp) (yes this is really how messy computer vision code can get if you want to squeeze out every last bit of performance from your computer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV encodes a color pixel with a BGR triple of values whereas Matplotlib and skimage use RGB triplets. That is why in the code below you see the cryptic ``f1[:,:,::-1]`` indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = cv2.SIFT_create()\n",
    "kp1, d1 = fd.detectAndCompute(f1[:,:,::-1], None)\n",
    "kp2, d2 = fd.detectAndCompute(f2[:,:,::-1], None)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(f1)\n",
    "draw_keypoints(kp1)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(f2)\n",
    "draw_keypoints(kp2)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have computed the SIFT descriptors, you can use those to establish possible matches. Matching points (one keypoint in image 1 and one in image 2) are based on the similarity of the descriptors. So a keypoint \n",
    "$(x,y,s,\\theta, \\v d)$ in image 1 and a keypoint $(x',y',s',\\theta',\\v d')$ in image 2 are considered a match in case $\\|\\v d-\\v d'\\|$ is small.\n",
    "\n",
    "We will use the 'brute force matcher' from OpenCV. It is a greedy algorithm: select the pair of keypoints, one from image 1 and one from image 2, with the minimal descriptor distance. This is the first match and these keypoints will not be considered again. Then you repeat the search and so on and so on, until you run out of descriptors in one of the two images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = cv2.BFMatcher()\n",
    "matches = matcher.match(d1,d2)\n",
    "matches = sorted(matches, key = lambda x: x.distance)\n",
    "\n",
    "draw_matches(f1, kp1, f2, kp2, matches[:200], figsize=(15,15), horizontal=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that you see a lot of 'wrong' matches in the above images. But are they really wrong? What is happening? Why do we find these 'wrong' matches? And what do we mean with 'wrong' really? \n",
    "\n",
    "These 'wrong' matches will be filtered out in the next section. The filtering will *not* be done based on visual appearance but on the possibility to find a common mapping of points in image 1 to the corresponding points in image 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are interested in algorithms and datastructures it might be interesting to realize that such a brute force approach to matching (essentially the problem of finding closest points in high dimensional space, in our case $\\v d\\in\\setR^{256}$) is rather slow. It turns out that a faster search for the closest point is not trivial. In OpenCV an *approximate* but faster algorithm is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample Concensus (RANSAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANSAC is a method to simulatenously estimate the parameters of a transformation given examples of (input, output) pairs while at the same time removing those pairs that are not related by the model (the *outliers*).\n",
    "\n",
    "The RANSAC algorithm needs two functions ``fit_model`` and ``test_model`` and proceeds as follows:\n",
    "\n",
    "1. The ``fit_model`` function takes a randomly selected list of ``n_model_data`` (input, output) pairs and estimates the parameters of the model, so that the model maps each of the inputs as close as possible to the corresponding outputs. Most often ``n_model_data`` is taken to be the minimum number of (input, output) pairs that allow the parameters to be calculated.\n",
    "\n",
    "1. The ``test_model`` function tests a given model on all available (input, output) pairs. It selects those pairs that fit the model quite good (a threshold to be set by the programmer, depending on what model is being used) as the *inliers*.\n",
    "\n",
    "1. The functions ``fit_model`` and ``test_model`` are called ``n_iter`` times each time with a random choice of ``n_model_data`` (input, output) pairs. The model with the most inliers is taken to be the optimal model.\n",
    "\n",
    "1. Finally, the model parameters are then estimated once more, not with just the ``n_model_data`` pairs, but with all inliers for this model.\n",
    "\n",
    "Read about the RANSAC algorithm (e.g. on Wikipedia) and complete the ``ransac`` function below. To see the ``ransac`` function in action (if you have completed the code, or when you are looking for an example in what way to write the ``fit_model`` and ``test_model`` function look at the subsequent section on line fitting with RANSAC.\n",
    "\n",
    "Your RANSAC solution should be reasonably fast: the test cell below should run within a few seconds. One of the codegrade tests will call the function repeatedly to test both its speed and correctness.\n",
    "\n",
    "Note that RANSAC is agnostic about the model type used, as long as it is consistent between the data and the functions `fit_model` and `test_model`. This allows us to write it once and use it on a variety of problems: in this notebook, line fitting and image alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0facf67cb1460f1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ransac(data, fit_model, test_model, test_model_pars, \n",
    "           n_model_data, n_iter):\n",
    "    \"\"\"\n",
    "    Use of RANSAC to fit a model to data\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    data: list\n",
    "       each item in data is a tuple (input, output)\n",
    "    fit_model: callable\n",
    "       a function that takes data (a list of tuples of \n",
    "       (input, output) pairs) and returns an object describing \n",
    "       the fitted model\n",
    "    test_model: callable\n",
    "       a function that takes as first parameter the model \n",
    "       (return value of fit_model) and as second argument a list\n",
    "       of (input, output) tuples (it expects all data tuples \n",
    "       including those used for the estimation). \n",
    "       It returns an array with the indices in the list of tuples \n",
    "       (second parameter) of all data tuples that fit the model \n",
    "       (i.e. the inliers)\n",
    "    test_model_pars: dict\n",
    "       a dictionary that is passed to the ``test_model`` function used to set\n",
    "       a threshold to determine if an (input, output) pair is an inlier or not.\n",
    "    n_model_data: int\n",
    "       the number of data tuples to be used to fit the model\n",
    "    n_iter: int\n",
    "       number of iterations \n",
    "    \n",
    "    \n",
    "    Return Value\n",
    "    ============\n",
    "    (model, inliers): tuple\n",
    "       model: the model, as returned by fit_model\n",
    "       inliers: the indices of the inliers of the returned model \n",
    "           (not just the data points used to fit the model)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE (Replace this and the following line with your code)\n",
    "    raise NotImplementedError()\n",
    "    return (model, inliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Ransac for Line Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try the RANSAC algorithm on the line fitting problem. Note: fitting a line is NOT what is needed in the image stitching problem! We start with it because it is an easier problem and more easily visualized. \n",
    "\n",
    "First we generate some noisy data. We generate random  洧논  values in the range from 0 to 10. The majority of those  洧논  values get an  洧녽  value (to generate the pair  (洧논,洧녽) ) that is on a straight line but with a random error. The remaining  洧논  values are given a random  洧녽 -value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "x = 10 * np.random.randn(70);\n",
    "y = 0.3 * x + 3.4\n",
    "y = y + 0.3 * np.random.randn(len(y))\n",
    "y[40:] = 8 * np.random.rand(30)\n",
    "plt.plot(x,y,'o');\n",
    "data = list(zip(x,y)) # a list of pairs (input,output), \n",
    "                      # in this case (x,y) pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given a number of $n>2$ pairs of points $(x,y)$ we can fit a line using a LSQ procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit_line(data):\n",
    "    \"\"\"Fit a line to (x,y) pairs using a LSQ procedure\"\"\"\n",
    "    X = np.array( [ [a, 1] for (a,b) in data])\n",
    "    y = np.array( [ b for (a,b) in data])\n",
    "    return np.linalg.lstsq(X, y, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "line_lsq = fit_line(data)\n",
    "yfit_lsq = line_lsq[0]*x + line_lsq[1]\n",
    "plt.plot(x,y,'o')\n",
    "plt.plot(x,yfit_lsq)\n",
    "plt.title('LSQ fit');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Evidently this is not the line we are looking for. We need to get rid of the outliers, i.e. the points $(x,y)$  that are not from the straight line. With the ``test_line`` function we can find the inliers for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_line(line, data, max_error=1):\n",
    "    x = np.array( [ x for x, _ in data])\n",
    "    y = np.array( [ y for _, y in data])\n",
    "    yp = line[0]*x + line[1]\n",
    "    err = np.abs(y-yp)\n",
    "    inliers_idx = np.flatnonzero(err<max_error)\n",
    "    return inliers_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Visual test (for the first test case below)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "me = 0.7\n",
    "line = None\n",
    "inliers_idx = None\n",
    "line, inliers_idx = ransac(data, fit_line, test_line, \n",
    "                           {'max_error':me}, 4, 1000)\n",
    "plt.plot(x, y, 'o');\n",
    "plt.plot(x[inliers_idx], y[inliers_idx], 'gx');\n",
    "yfit = line[0]*x + line[1]\n",
    "plt.plot(x, 0.3 * x + 3.4, label='True Line')\n",
    "plt.plot(x,yfit, label='Fitted Line')\n",
    "plt.plot(x,yfit+me, 'lightgray')\n",
    "plt.plot(x,yfit-me, 'lightgray')\n",
    "plt.plot(x,yfit_lsq, label='LSQ fit')\n",
    "plt.legend()\n",
    "plt.title('RANSAC fit vs LSQ fit');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a Perspective Transform using Ransac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember where we are in the stitching process? We have found keypoints in the first and second image, collected in arrays ``kp1`` and ``kp2``. The matching pairs are encoded in the ``matches`` datastructure. First we are going to make a ``data`` list where each entry is of the form ``((x1, y1), (x2, y2))`` that is a tuple of two tuples encoding the position of matching keypoint coordinates in the first and second image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [((kp1[m.queryIdx].pt), (kp2[m.trainIdx].pt)) for m in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a ``fit_model`` and ``test_model`` function in order to use Ransac. You have already defined ``projective_fit_model`` which takes a set of point correspondences and finds the best projective matrix ``P`` that maps points (x1, y1) to (x2, y2). Now define `projective_test_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e05c65c8aa46e579",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def projective_test_model(P, data, max_error=2):\n",
    "    \"\"\"\n",
    "    Finds indices of inliers in the data for projective transformation\n",
    "    given by P, where the inliers are determined by the max_error.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    P: numpy array\n",
    "       a 3x3 matrix giving a projective transformation of the plane\n",
    "    data: list\n",
    "       each item in data is a tuple (input, output), each consisting\n",
    "       of a point (numpy array with shape (2,)) in the plane.\n",
    "    max_error: float \n",
    "       the maximum Euclidean distance between P(input) and output \n",
    "       for the pair (input, output) to count as an inlier\n",
    "    \n",
    "    Return Value\n",
    "    ============\n",
    "    inliers: numpy array\n",
    "       the indices of the inliers in data, in order\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE (Replace this and the following line with your code)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the assert statements in this cell to locally test your projective_test_model.\n",
    "# There will be other tests in the codegrade autograding.\n",
    "\n",
    "x = np.array([[-0.30099881,  0.34786734],\n",
    "       [-0.43573294,  0.64231348],\n",
    "       [-1.28532622,  1.13860487],\n",
    "       [ 0.54744592,  0.51493255],\n",
    "       [-0.20087976,  0.18980669],\n",
    "       [-1.27066563, -0.20241131],\n",
    "       [-0.64972417, -2.02646797],\n",
    "       [-2.38189517, -0.1663487 ]])\n",
    "xa = np.array([[ 0.369618  ,  0.67714101],\n",
    "       [ 1.72259747, -0.04050429],\n",
    "       [ 2.22190863, -1.06356781],\n",
    "       [ 0.80900945, -2.48119838],\n",
    "       [-0.53911635,  1.2555933 ],\n",
    "       [ 0.07221467, -0.24165753],\n",
    "       [-2.37579957,  0.87320773],\n",
    "       [ 0.47265383, -0.59226317]])\n",
    "P = np.array([[ 1.        , -1.8574528 ,  0.77686397],\n",
    "                [-1.0861612 ,  0.63049228, -0.85821791],\n",
    "                [ 1.01049866,  0.34736811, -0.27737639]])\n",
    "assert np.array_equal(projective_test_model(P, list(zip(x, xa))), np.arange(8))\n",
    "xa[::2, 0] += 1\n",
    "xa[::4, 0] += 1\n",
    "xa[::3, 1] += 1\n",
    "xa[1::3, 1] += 1\n",
    "assert np.array_equal(projective_test_model(P, list(zip(x, xa)), 0.5), np.array([5]))\n",
    "assert np.array_equal(projective_test_model(P, list(zip(x, xa)), 1.1), np.array([1, 2, 3, 5, 7]))\n",
    "assert np.array_equal(projective_test_model(P, list(zip(x, xa)), 2), np.array([1, 2, 3, 5, 6, 7]))\n",
    "assert np.array_equal(projective_test_model(P, list(zip(x, xa)), 3), np.arange(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, inliers = ransac(data, projective_fit_model, \n",
    "                    projective_test_model, {'max_error':1}, 4, 1000) \n",
    "print(np.linalg.inv(P))\n",
    "draw_matches(f1, kp1, f2, kp2, np.array(matches)[inliers], \n",
    "             horizontal=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stitching it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "f_stitched = warp(f2, P, output_shape=(300,450), preserve_range=True).astype(np.uint8)\n",
    "M, N = f1.shape[:2]\n",
    "f_stitched[:M, :N, :] = f1\n",
    "plt.imshow(f_stitched);\n",
    "plt.axis('off');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) A Better Stitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitching the images was done by\n",
    "\n",
    "1. warping the f2 image onto the coordinate frame of the first image (using a larger image than f1 to fit the second one) (1pt)\n",
    "\n",
    "1. copying the first image to this larger image (as the top left content) (1pt)\n",
    "\n",
    "This was all done by trial and error. But as you can see some parts of the second image are not visible in the resulting larger image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c5fc86de9243312",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Your task now is to define an image in which both images fit entirely. From the image above we see that we need some more spave at the top and bottom. The easiest way (for me) is to calculate the bounding box of the transformed second image (i.e. based on the coordinates of the 4 corners of the second image with respect to the coordinate frame of the first image). Use your knowledge of coordinate transforms and homogeneous transforms (you need a translation matrix...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Stitching your own Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5a3cce3340017928",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "We challenge you to take a few pictures of the same scene that do overlap and try to stitch them together in one large image.\n",
    "\n",
    "1. Start with two images! Be aware that not all two images from the same scene can be stitched together. As a rule of thumb: take pictures of an almost flat surface (like the Nachtwacht or a flat facade of a building, or pictures of a poster on your wall) or take pictures while keeping your camera at one spot and only rotating the camera (like taking a picture of a mountain view at a distance far from the mountains). (2pt)\n",
    "\n",
    "1. Then try three images. Start with the a priori knowledge that f1 can be stitched with f2 and f2 with f3. (1pt)\n",
    "\n",
    "1. If you are really brave, try 5 or 6 images. The record for this class is 5 images... (no points but a lot of bragging rights)\n",
    "\n",
    "When using real life images you undoubtly encounter the need for color correction. The images used in stitching do not show the same colors at corresponding points (due to lighting mostly). Color correction is nice subject that we don't have the time to deal with in these lecture series, so in particular it is not part of this bonus assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Using your own images\n",
    "\n",
    "**Note, when doing an assignment with your own images, you have to upload those along with the notebook!**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
